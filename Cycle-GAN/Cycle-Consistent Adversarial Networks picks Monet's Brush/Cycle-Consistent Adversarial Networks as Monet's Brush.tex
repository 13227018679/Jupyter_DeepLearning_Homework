\documentclass[a4paper]{article}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{indentfirst}
\usepackage{float}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts,amssymb}
\usepackage{hyperref}
\usepackage{listings}
\usepackage[top=2cm,left=2.85cm,right=2.85cm,bottom=2.6cm]{geometry}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\renewcommand{\baselinestretch}{1.2}
\setlength{\parskip}{0.5\baselineskip}
\captionsetup[figure]{labelfont={bf},name={Fig.},labelsep=period}
\usepackage{subfig}

\begin{document}
\vspace{6mm}
\begin{center}
\huge\textbf{Cycle-Consistent Adversarial Network \\}
\vspace{3mm}
\huge\textbf{picks Monet's Brush}
\end{center}
\vspace{3mm}

\begin{figure}[H]
\centering
\includegraphics[width=8cm,height=8cm]{sysu.png}
\end{figure}

\vspace{2mm}
\begin{center}
\large\textbf{16340154 -- Nino Lau }\\
\end{center}

\begin{center}
\normalsize{School of Data and Computer Science, Sun Yat-sen University}
\end{center}

\begin{center}
\textit{April. $27^{nd}$ \textit, 2019\\}
\end{center}
\vspace{5mm}

\noindent \HRule
\vspace{2.5mm} \\
\large{As we have deep into reinforce learning, structured learning has become a common topic. Generative Adversary Network (\textsf{GAN}) is a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model that captures the data distribution, and a discriminative model that estimates the probability that a sample came from the training data. However, considering some limitations of \textsf{GAN}, such as paired training data requirement, we need a more appropriate model for style transfer. Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. Paper \href{https://arxiv.org/pdf/1703.10593.pdf}{\emph{``Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks''}} presents an approach (\textsf{Cycle-GAN}) for learning to translate an image from a source domain to a target domain in the absence of paired examples. Quantitative comparisons against prior methods demonstrate the superiority of their approach.}
\vspace{2mm} \\
\HRule

%%
\vspace{6mm}
\begin{center}
\LARGE\textbf{I. Background} \\
\end{center}
\vspace{2mm}

\large{Various techniques have been applied to image style transfer area, such as Computer Vision and CNN. However, all have limitation when facing with different situations so far. In order to build a high-performance image-to-image translation model, the authors in the paper utilize the latest scientific achievements and derive the essence of the following state-of-art methods to implement their \textsf{Cycle-GAN} model. }

\begin{center}
\large\textbf{Generative Adversary Network} \\
\end{center}

\large
As one of the family member of \textsf{GAN} family, it's nature for us to understand the whole principles of fundamental \textsf{GAN} before we step into \textsf{Cycle-GAN}. \href{https://github.com/hindupuravinash/the-gan-zoo}{\emph{\textsf{GAN} Zoo contains over 500 members}}, indicating the popularity of this novel model. 

\vspace{3mm}
\begin{figure}[h]
\centering
\includegraphics[width=12.5cm,height=5cm]{GAN.png}
\caption{\textsf{GAN} architecture}
\end{figure}
\vspace{3mm}

In the proposed adversarial nets framework, the generative model is pitted against an adversary: a discriminative model that learns to determine whether a sample is from the model distribution or the data distribution. The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine articles. 

A generator \texttt{G} and a discriminator \texttt{D} in \textsf{GAN} model play a zero-sum game. The training process is formulated as follows,
\begin{equation}
\arg \mathop{\min}_{G}\mathop{\max}_{D}V(D,G)=\mathbb{E}_{ \boldsymbol x\sim p_{data}(\boldsymbol x)} [\log D(\boldsymbol x)] + \mathbb{E}_{ \boldsymbol z\sim p_{ \boldsymbol z}(\boldsymbol z)} [\log {(1-D(G(\boldsymbol z)))}].
\end{equation}

The whole algorithm is listed as follows.

\begin{figure}[h]
\centering
\includegraphics[width=14.8cm,height=9.75cm]{GANalg.png}
\end{figure}

Results in this paper demonstrates the viability of the adversarial modeling framework, suggesting that these research directions could prove useful.

\vspace{2mm}
\begin{center}
\large\textbf{Conditional Adversarial Networks}
\end{center}
\vspace{1mm}
\large{

Conditional generation is an enhancement of traditional models, including additional input conditional variables. The image-conditional models have tackled image prediction from a normal map, future frame prediction, product photo generation, and image generation from sparse annotations. Paper \href{https://arxiv.org/pdf/1611.07004.pdf}{\emph{Image-to-Image Translation with Conditional Adversarial Networks}} investigates conditional adversarial networks to image-to-image translation as a pioneer. The model in that paper can not only learn the mapping from input image to output image, but also learn a loss function to train this mapping problems. Widely-know as \textsf{Pix2Pix}, it's sufficient to generate good results.

\vspace{2mm}
\begin{figure}[H]
\centering
\includegraphics[width=12.5cm,height=4.58cm]{pix2pix.png}
\caption{\textsf{Pix2Pix} model for image-to-image transition}
\end{figure}
\vspace{2mm}

The principle behind is not sophisticated. The formula is re-defined as follows, 

\begin{equation}
\mathcal{L}_c GAN(G,D)=\mathbb{E}_{x,y} [\log D(x,y)] + \mathbb{E}_{x,z} [\log {(1-D(x,G(x,z))}].
\end{equation}

\begin{equation}
\mathcal{L}_{L1}(G)=\mathbb{E}_{x,y,z} [\|\log {(y-G(x,z))}\|_1].
\end{equation}

\begin{equation}
\arg \mathop{\min}_{G}\mathop{\max}_{D}\mathcal{L}_c GAN(G,D)+\lambda \mathcal{L}_{L1}(G).
\end{equation}
\vspace{2mm}

Results suggest that \textsf{Pix2Pix} are a promising approach for many image-to-image translation tasks, especially those involving highly structured graphical outputs. \textsf{Pix2Pix} learns a loss adapted to the task and data at hand, which makes them applicable in a wide variety of settings.

}


\vspace{5mm}
\begin{center}
\large\textbf{Cycle Consistency} \\
\end{center}

\large{
The idea of using transitivity as a way to regularize structured data has a long history. In visual tracking, enforcing simple forward-backward consistency has been a standard trick for decades. In the language domain, verifying and improving translations via “\emph{back translation and reconciliation}” is a technique used by human translators, as well as by machines. Paper \href{http://openaccess.thecvf.com/content_cvpr_2016/papers/Zhou_Learning_Dense_Correspondence_CVPR_2016_paper.pdf}{\emph{Learning Dense Correspondence via 3D-guided Cycle Consistency}} establishes dense visual correspondence across different object instances.}

\vspace{3mm}
\begin{figure}[H]
\centering
\includegraphics[width=12.5cm,height=4.1cm]{cycleconsistence.png}
\caption{Learning Dense Correspondence via 3D-guided Cycle Consistency}
\end{figure}
\vspace{2mm}

\clearpage

%%
\vspace{5mm}
\begin{center}
\LARGE\textbf{II. Model} \\
\end{center}
\vspace{2mm}

\vspace{2mm}
\begin{center}
\large\textbf{Architecture} \\
\end{center}


\large{
Aim to capture special characteristics of one image collection and figuring out how these characteristics could be translated into the other image collection, all in the absence of any paired training examples, the paper tries to exploit an algorithm that can learn to translate between domains without paired input-output examples. The framework is shown as \textbf{Fig. \ref{cyclegan}}.

\vspace{3mm}
\begin{figure}[H]
\centering
\includegraphics[width=12.5cm,height=6.66cm]{cyclegan.png}
\caption{\textsf{Cycle-GAN architecture}}
\label{cyclegan}
\end{figure}
}

%%%%%
\vspace{5mm}
\begin{center}
\large\textbf{Algorithm} \\
\end{center}

\large{Our goal is to learn mapping functions between two domains $X$ and $Y$. We denote the data distribution as $\boldsymbol x \sim p_{data}(\boldsymbol x)$ and $\boldsymbol y \sim p_{data}(\boldsymbol y)$. \textsf{Cycle-GAN} model includes 2 mappings: $G:X \rightarrow Y$ and $F: Y\rightarrow X$. \textsf{Cycle-GAN} introduces 2 adversarial discriminators $D_X$ and $D_Y$, where $D_X$ aims to distinguish between images $\{x\}$ and translated images $\{ F(y) \}$; in the same way, $D_Y$ aims to discriminate between $\{y\}$ and $\{G(x)\}$. Our objective contains two types of terms: 

\begin{itemize}
    \item \textbf{Adversarial losses:} matches the distribution of generated images to the data distribution in the target domain.
    \item \textbf{Cycle Consistency losses:} serves to prevent the learned
mappings G and F from contradicting each other.
\end{itemize}

\begin{align}
    \mathcal{L}_{GAN}(G,D_y,X,Y)=&\mathbb{E}_{ \boldsymbol y\sim p_{data}(\boldsymbol y)} [\log D_Y(\boldsymbol y)] \notag \\
    &+ \mathbb{E}_{ \boldsymbol x\sim p_{data}(\boldsymbol x)} [\log {(1-D_Y(G(\boldsymbol x)))}].
\end{align}
\begin{align}
    \mathcal{L}_{cyc}(G,F)=&\mathbb{E}_{ \boldsymbol x\sim p_{data}(\boldsymbol x)} [\|F(G(\boldsymbol x))-\boldsymbol x\|_1] \notag \\ 
    &+ \mathbb{E}_{ \boldsymbol y\sim p_{data}(\boldsymbol y)} [\|G(F(\boldsymbol y))-\boldsymbol y\|_1].
\end{align}
\begin{align}
    \arg \mathop{\min}_{G,F}\mathop{\max}_{D_{X},D_{Y}}\mathcal{L}(G,F,D_X,D_Y)=&\mathcal{L}_{GAN}(G,D_y,X,Y) \notag\\
    &+\mathcal{L}_{GAN}(G,D_X,Y,X)\notag\\
    &+\mathcal{L}_{cyc}(G,F).
\end{align}

}

\vspace{5mm}
\begin{center}
\LARGE\textbf{III. Experiments} \\
\end{center}
\vspace{2mm}

\vspace{2mm}
\begin{center}
\large\textbf{Metrics} \\
\end{center}

\large{We keep an image buffer that stores the 50 previously created images. For all the experiments, we set $\lambda$ = 10. We use the Adam solver with SGD. All networks were trained from scratch with a learning rate of 0.0002. We keep the same learning rate for the first 100 epochs and linearly decay the rate to zero over the next 100 epochs.}

%%%%%
\vspace{2mm}
\begin{center}
\large\textbf{Implementation} \\
\end{center}

\large{Our whole implementation in \textbf{Appendix.} A.

The network contains two stride-2 convolutions, several residual blocks, and two fractionally strided convolutions with stride .5. We use 6 blocks for 128 * 128 images and 9 blocks for 256 * 256 and higher resolution training images. Similar to Johnson et al., we use instance normalization. For the discriminator networks we use 70 * 70 \textsf{Patch-GANs}, which aim to classify whether 70 * 70 overlapping image patches are real or fake.}

%%%%%
\vspace{2mm}
\begin{center}
\large\textbf{Results} \\
\end{center}

\large{More results please view in \textbf{Appendix.} B.

We conduct our experiment on 5 datasets, and we use monet2photo dataset as a illustration here.

\vspace{3mm}
\begin{figure}[H]
\centering
\includegraphics[width=13.5cm,height=8.2cm]{losses.pdf}
\caption{Results with training (Monet painting versus photograph)}
\label{losses}
\end{figure}
\vspace{2mm}

\vspace{3mm}
\begin{figure}[H]
\centering
\includegraphics[width=13.5cm,height=8.2cm]{training.png}
\caption{Results with training (Monet painting versus photograph)}
\label{resultswithtraining}
\end{figure}
\vspace{2mm}

\textbf{Fig. \ref{losses} }shows the loss of generator and discriminator in 200 epoch respectively. Note that the growth trend of the two curves is opposite, indicating the antagonism between them. \textbf{Fig. \ref{resultswithtraining} }shows the style transfer between Monet's painting and real photograph with the training. The first and third line list dataset images and the second and forth line give the results correspondingly. Results become more accurate with the training, changing from random noise to fakes which are difficult to identify. Experiments show that both of the losses decrease, demonstrating the effectiveness of \textsf{Cycle-GAN}.


} 

%%
\vspace{15mm}
\begin{center}
\LARGE\textbf{IV. Conclusion} \\
\end{center}
\vspace{2mm}

In this report, we implement the code of \href{https://arxiv.org/pdf/1703.10593.pdf}{\emph{``Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks''}}. \textsf{Cycle-GAN} stands on the shoulder of some splendid previous work show great performance when applied to style transaction. We modify the original code of the paper and train the model for about a month. We born the midnight oil and eventually get the trained models and results. This experiment of reproducing give us a deeper understanding of \textsf{GANs} and encourage us to devote to deep learning the whole life.

%%
\vspace{1.5cm}
\begin{center}
\LARGE\textbf{V. Acknowledgement} \\
\end{center}
\vspace{.5mm}

\begin{itemize} \item{First and foremost, I would like to show my deepest gratitude to my teacher, professor \textbf{Roy Wang}, who has provided us with valuable theoretical guidance of deep learning. Without his enlightening instruction, impressive kindness and patience, I could never have completed our projects. His keen and vigorous academic observation enlightens me in this thesis and my future study.}
\item{My sincere appreciation also goes to our \textbf{teacher assistants}. We got along in harmony, accomplished tasks together, and they give me valuable advice. All of these make this experience meaningful.}
\end{itemize}

\clearpage
%%
\vspace{5mm}
\begin{center}
\LARGE\textbf{VI. Appendices} \\
\end{center}
\vspace{1mm}

\vspace{5mm}
\begin{center}
\large\textbf{A. Code Implementation} \\
\end{center}


\vspace{2mm}
\large\noindent{
Code implementation is a fine-tuning, referring \href{https://github.com/hindupuravinash/the-gan-zoo}{\underline{\textsf{GAN} Zoo}} and \href{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix}{\underline{Paper Implementation}}.
}
\vspace{1cm}

\noindent \LARGE \textsc{1) cycleGAN.py} \small

\noindent \HRule

\lstinputlisting[language=python]{./cycleGAN.py}

\noindent \HRule

\vspace{1.5cm}

\noindent \LARGE \textsc{2) models.py} \small

\noindent \HRule

\lstinputlisting[language=python]{./models.py}

\noindent \HRule

\vspace{1.5cm}

\noindent \LARGE \textsc{3) utils.py} \small

\noindent \HRule

\lstinputlisting[language=python]{./utils.py}

\noindent \HRule

\vspace{1.5cm}

\noindent \LARGE \textsc{4) datasets.py} \small

\noindent \HRule

\lstinputlisting[language=python]{./datasets.py} 

\noindent \HRule

\vspace{2cm}

\begin{center}
\LARGE\textbf{B. More Results} \\
\end{center}

\vspace{3mm}
\begin{figure}[H]
\centering
\includegraphics[width=13.5cm,height=14.5cm]{cycleGAN.pdf}
\caption{Results with training in various datasets.}
\end{figure}
\vspace{2mm}

\end{document}
